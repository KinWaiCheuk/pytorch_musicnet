{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal, sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import conv1d\n",
    "import torchvision\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "from time import time\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "import musicnet\n",
    "# from helperfunctions import get_audio_segment, get_piano_roll, export_midi\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'   # see issue #152\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pypianoroll import Multitrack, Track, load, parse\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvl1 convolutions are shared between regions\n",
    "m = 128\n",
    "k = 512              # lvl1 nodes\n",
    "n_fft = 4096              # lvl1 receptive field\n",
    "window = 16384 # total number of audio samples?\n",
    "stride = 512\n",
    "batch_size = 100\n",
    "epsilon = 1e-8\n",
    "\n",
    "regions = 1 + (window - n_fft)//stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, avg=.9998):\n",
    "        super(CNN, self).__init__()      \n",
    "        # Create filter windows\n",
    "        wsin, wcos = musicnet.create_filters(n_fft,k, low=50, high=6000,\n",
    "                                      windowing=\"hann\", freq_scale='log')\n",
    "        self.wsin = torch.Tensor(wsin)\n",
    "        self.wcos = torch.Tensor(wcos)               \n",
    "        # Creating Layers\n",
    "        \n",
    "        k_out = 128\n",
    "        k2_out = 256\n",
    "        self.CNN_freq = nn.Conv2d(1,k_out,\n",
    "                                kernel_size=(128,1),stride=(2,1))\n",
    "        self.CNN_time = nn.Conv2d(k_out,k2_out,\n",
    "                                kernel_size=(1,25),stride=(1,1))        \n",
    "        self.linear = torch.nn.Linear(k2_out*193, m, bias=False)\n",
    "\n",
    "        # Initialize weights\n",
    "            # Do something\n",
    "        \n",
    "    def forward(self,x):\n",
    "        zx = conv1d(x[:,None,:], self.wsin, stride=stride).pow(2) \\\n",
    "           + conv1d(x[:,None,:], self.wcos, stride=stride).pow(2) # shape = (batch, 512,25)\n",
    "        zx = torch.log(zx + 1e-12)\n",
    "        z2 = torch.relu(self.CNN_freq(zx.unsqueeze(1))) # Make channel as 1 (N,C,H,W) shape = [10, 128, 193, 25]\n",
    "        z3 = torch.relu(self.CNN_time(z2)) # shape = [10, 256, 193, 1]\n",
    "        y = self.linear(torch.relu(torch.flatten(z3,1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../weights/translation_invariant_baseline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_full(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        x = np.fromfile(f, dtype=np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piano_roll_from_wav(filepath, model, device, window=16384, stride=1000, offset=44100, count=7500, batch_size=500, m=128):\n",
    "    sf=4\n",
    "    x = read(filepath)[1]\n",
    "    if x.ndim==2:\n",
    "        x = x.mean(1) # convert stereo to mono\n",
    "    elif x.ndim>2:\n",
    "        print(\"the audio shape {} is not correct, please check and fix it\".format(x.shape))\n",
    "    \n",
    "    if stride == -1:\n",
    "        stride = (x.shape[0] - offset - int(sf*window))/(count-1)\n",
    "        stride = int(stride)\n",
    "        print(\"Number of stride = \", stride)\n",
    "    else:\n",
    "        count = (x.shape[0]- offset - int(sf*window))/stride + 1\n",
    "        count = int(count)\n",
    "        \n",
    "    X = np.zeros([count, window])\n",
    "    Y = np.zeros([count, m])    \n",
    "        \n",
    "    for i in range(count):\n",
    "        temp =  x[offset+i*stride:offset+i*stride+window]\n",
    "        temp = temp / (np.linalg.norm(temp) + epsilon)\n",
    "        X[i,:] = temp\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        Y_pred = torch.zeros([count,m])\n",
    "        for i in range(len(X)//batch_size):\n",
    "            print(f\"{i}/{(len(X)//batch_size)} batches\", end = '\\r')\n",
    "            X_batch = torch.tensor(X[batch_size*i:batch_size*(i+1)]).float().to(device)\n",
    "            Y_pred[batch_size*i:batch_size*(i+1)] = model(X_batch).cpu()\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_midi(Y_pred, path):\n",
    "    # Create a piano-roll matrix, where the first and second axes represent time\n",
    "    # and pitch, respectively, and assign a C major chord to the piano-roll\n",
    "    # Create a `pypianoroll.Track` instance\n",
    "    track = Track(pianoroll=Y_pred*127, program=0, is_drum=False,\n",
    "                  name='my awesome piano')   \n",
    "    multitrack = Multitrack(tracks=[track], tempo=60, beat_resolution=86)\n",
    "    multitrack.write(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './'\n",
    "files = ['BWV846.wav','BWV972.wav','2.wav', '3.wav']\n",
    "filepath_list = [os.path.join(folder, i) for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in filepath_list:\n",
    "    Y_pred = get_piano_roll_from_wav(filepath, model, device,\n",
    "                                window=window, m=m, stride=512)\n",
    "    Yhatpred = Y_pred.cpu().numpy() > 0.4\n",
    "    export_midi(Yhatpred, './midi_output/{}_{}.mid'.format('CNN_',os.path.basename(filepath)[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
